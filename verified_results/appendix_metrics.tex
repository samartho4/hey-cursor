\section{Appendix: Detailed Metrics and Reproducibility}

\subsection{Statistical Analysis Details}

\paragraph{Bootstrap BCa confidence intervals.} We compute bias-corrected and accelerated (BCa) confidence intervals using $B=10,000$ bootstrap replicates with fixed random seed (seed=42). The BCa method provides second-order accurate intervals that correct for bias and skewness in the bootstrap distribution. % source: results/statistical_analysis.json

\paragraph{Wilcoxon signed-rank test.} We employ paired, two-sided Wilcoxon signed-rank tests for robustness against non-normal error distributions. The test statistic and p-value are computed using scipy.stats.wilcoxon with the 'two-sided' alternative. % source: results/statistical_analysis.json

\paragraph{Effect size calculations.} Cohen's $d_z$ for paired differences accounts for the correlation between paired observations: $d_z = \frac{\mu_{\Delta}}{\sigma_{\Delta}}$, where $\mu_{\Delta}$ is the mean difference and $\sigma_{\Delta}$ is the standard deviation of differences. % source: results/statistical_analysis.json

\subsection{Computational Environment and Reproducibility}

\paragraph{Software versions and hardware.} All experiments conducted on macOS 24.3.0 with Python 3.13.4, Julia 1.11.6, and the following key packages: numpy 2.3.2, scipy 1.16.1, pandas 2.3.2, matplotlib 3.10.5. No GPU acceleration was used. % source: results/terminal_checks.txt

\paragraph{Random seeds and deterministic execution.} All experiments use fixed random seed (seed=42) for reproducibility. Git commit hash: 320bb9f01532139e375a9d80b9b14e333383c664. % source: results/terminal_checks.txt

\paragraph{Solver tolerances and numerical stability.} All differential equation solvers use Rosenbrock23 with relative tolerance $10^{-7}$ and absolute tolerance $10^{-9}$. A tolerance sweep confirmed that reducing tolerances further produces negligible changes in RMSE (<0.002). % source: results/terminal_checks.txt

\subsection{Model Configurations and Hyperparameters}

\paragraph{UDE configuration.} Neural network: 3 hidden units, tanh activation, L2 regularization $\lambda = 10^{-6}$. Training: L-BFGS optimization with composite loss combining RMSE, MAPE (weight 0.7), and L2 penalty. % source: results/terminal_checks.txt

\paragraph{BNODE configuration.} MCMC: 4 chains, 1000 samples each, NUTS sampler. Priors: Gaussian weights ($\sigma^2 = 0.1$), Student-$t$ likelihood ($\nu = 3$). Calibration: post-hoc variance scaling on validation set. % source: results/simple_bnode_calibration_summary.md

\subsection{Additional Metrics and Diagnostics}

\paragraph{MCMC convergence diagnostics.} Effective sample sizes average 333 across all parameters. R-hat calculation encountered technical issues (NaN values) due to implementation constraints. % source: results/bnode_mcmc_diagnostics.csv

\paragraph{Runtime analysis methodology.} Inference times measured after JIT warm-up using 1000 calls per scenario. Memory usage tracked but not reported due to negligible differences. % source: results/runtime_analysis.csv

\paragraph{Symbolic extraction details.} Cubic polynomial fitting using least squares regression on UDE residual evaluations. Coefficients reported with 95\% confidence intervals from bootstrap resampling. % source: results/ude_symbolic_extraction.md

\subsection{Data Availability and Code Reproducibility}

\paragraph{Dataset splits.} Training: 50 scenarios, Validation: 10 scenarios, Test: 10 scenarios. All splits use scenario-level separation to prevent data leakage. % source: results/comprehensive_metrics.csv

\paragraph{Checkpoint availability.} Model checkpoints available in checkpoints/ directory with SHA256 hashes for verification. UDE checkpoint: fc3f2e0c203038a4d9f14c8e4c265aa7b898a4ea8e9c0f31cd852b09b96ccdb2. BNODE checkpoint: 3713ad5d30b82ef3f32cfac6a26ce6ffa42f62e0201d1868c56444c79f09e23c. % source: results/terminal_checks.txt

\paragraph{Script execution.} All analysis scripts are executable and produce the reported metrics. Key scripts: comprehensive_model_comparison.jl, bnode_train_calibrate.jl, generate_research_figures.jl. % source: results/terminal_checks.txt
